\documentclass[11pt,a4paper]{report}

\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[cp1250]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{savesym}
\savesymbol{arc}
\usepackage{color}
\usepackage{xcolor}
\usepackage{pict2e}
\usepackage{epstopdf}
\usepackage{geometry}
\usepackage{multirow}

\newgeometry{tmargin=1.5cm, bmargin=1.5cm, lmargin=1.5cm, rmargin=1.5cm}
\pagestyle{empty}
\linespread{1.7}

\begin{document}

\section*{\centering SPRAWOZDANIE 3 \\ EKONOMETRIA \\ MARTA SOMMER -- BSMAD}

Bêdziemy rozwa¿aæ $16$ portfeli i na tej podstawie modelowaæ polsk¹ gie³dê. Mamy do dyspozycji dwa zestawy danych posortowane odpowiednio wzglêdem ró¿nych wskaŸników. Bêdziemy chcieli sprawdziæ, który model: jednoczynnikowy, trójczynnikowy, czy czteroczynnikowy, dobrze opisuje polsk¹ gie³dê.

\subsection*{Model jednoczynnikowy}

Rozwa¿my model jednoczynnikowy:
$$
R_{i,t}=\alpha_i + \beta_{RM,i}RM_t+\varepsilon_{i,t},
$$
gdzie $i=1,\ldots,16$, $t=1,\ldots,97$.

Dopasowaliœmy wiêc w ten sposób $16$ portfeli dla pierwszego zestawu danych.

Heteroskedastycznoœæ badaliœmy testem Breuscha-Pagana i ten nie wykaza³ heteroskedastycznoœci w ¿adnym modelu, natomiast test White'a wykry³ heteroskedastycznoœæ tylko w portfelu $16$. Œmia³o mo¿emy wiêc stwierdziæ, ¿e nie mamy tu do czynienia z heteroskedastycznoœci¹. Co do autokorelacji, to, przy pomocy testu Durbina-Watsona, zosta³a ona wykryta w ponad po³owie przypadków, czyli niestety mamy z ni¹ do czynienia w modelu. Zatem w portfelach, w których autokorelacja zosta³a wykryta bêdziemy przy szacowaniu wariancji korzystaæ z poprawki Newey-Westa.  

Zobaczmy teraz w jaki sposób uk³adaj¹ siê wartoœci estymatorów $\beta_{RM}$:

\bigskip

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{RM}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$}  & $ 0.9553296 $ & $ 0.8540825 $ & $ 0.8334983 $ & $ 0.8029792 $ \\ \cline{2-5} 
 & $ 1.008922 $ & $ 0.8993477 $ & $ 0.8797741 $ & $ 1.045466 $ \\ \cline{2-5} 
 & $ 1.1172 $ & $ 1.058734 $ & $ 1.002255 $ & $ 1.137929 $ \\ \cline{2-5} 
 & $ 1.09756 $ & $ 1.1529 $ & $ 1.152157 $ & $ 1.124232 $ \\ \hline
\end{tabular}
\end{center}

Z powy¿szej tabeli nie widaæ wiele zale¿noœci. Jedynie to, ¿e mniejsze spó³ki z wiêkszym $\dfrac{BV}{MV}$ (du¿a wartoœæ ksiêgowa i ma³a wartoœæ rynkowa) s¹ bardziej agresywne (a tym samym bardziej ryzykowne) ni¿ spó³ki z ma³ym wspó³czynnikiem $\dfrac{BV}{MV}$. 

\bigskip

PrzejdŸmy teraz do testu restrykcji GRS o hipotezie:

$$
\left\{
\begin{array}{l}
H: \alpha = 0 \\
K: \alpha \neq 0\\
\end{array}
\right.
$$

$P-value$ tego testu jest ma³e i wynosi $0,016$, zatem odrzucamy hipotezê zerow¹, czyli $\alpha \neq 0$. Oznacza to tyle, ¿e nasz model jednoczynnikowy Ÿle opisuje polsk¹ gie³dê, gdy¿ istnieje w nim element losowoœci, przypadkowoœci. Nie ma wiêc ju¿ sensu badaæ zestawu drugiego danych ani oszacowywaæ premii za ryzyko w modelu.


\subsection*{Model trójczynnikowy}

Rozwa¿my model trójczynnikowy:
$$
R_{i,t}=\alpha_i + \beta_{RM,i}RM_t+ \beta_{SMB,i}SMB_t+ \beta_{HML,i}HML_t+ \varepsilon_{i,t},
$$
gdzie $i=1,\ldots,16$, $t=1,\ldots,97$.

Dopasowaliœmy wiêc w ten sposób $16$ portfeli dla pierwszego zestawu danych.

Heteroskedastycznoœæ badaliœmy testem Breuscha-Pagana i ten wykaza³ heteroskedastycznoœæ w trzech portfelach, natomiast test White'a wykry³ heteroskedastycznoœæ w piêciu portfelach. Nale¿y wiêc stwierdziæ, ¿e heteroskedastycznoœæ jest obecna. Co do autokorelacji, to, przy pomocy testu Durbina-Watsona, zosta³a ona wykryta tylko w dwóch przypadkach, czyli problem autokorelacji w modelu mo¿emy pomin¹æ. Mimo wszystko, ze wzglêdu na heteroskedastycznoœæ, trzeba bêdzie skorzystaæ z poprawki Newey-Westa.  

Zobaczmy teraz w jaki sposób uk³adaj¹ siê wartoœci estymatorów $\beta$:

\bigskip

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{RM}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ 0.9385604 $ & $ 0.8875909 $ & $ 0.8255328 $ & $ 0.8495374 $ \\ \cline{2-5}
 & $ 0.8592244 $ & $ 0.8457644 $ & $ 0.821443 $ & $ 1.036485 $ \\ \cline{2-5}
 & $ 1.052344 $ & $ 0.9245519 $ & $ 0.8528089 $ & $ 1.08829 $ \\ \cline{2-5}
 & $ 0.8839021 $ & $ 0.8837371 $ & $ 0.9143578 $ & $ 0.9778986 $ \\ \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{SMB}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ 1.348105 $ & $ 1.107419 $ & $ 0.3920486 $ & $ 0.0130365 $ \\ \cline{2-5}
 & $ 1.481069 $ & $ 0.9689015 $ & $ 0.2761607 $ & $ 0.2071676 $ \\ \cline{2-5}
 & $ 1.322681 $ & $ 0.8095409 $ & $ 0.1186516 $ & $ -0.1282186 $ \\ \cline{2-5}
 & $ 1.176667 $ & $ 0.7678757 $ & $ 0.06623415 $ & $ -0.4334654 $ \\ \hline
\end{tabular}
\end{center}

Widaæ, ¿e im wiêksza spó³ka tym mniejsze ryzyko zwi¹zane z SMB.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{HML}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ -0.322925 $ & $ -0.4655263 $ & $ -0.0808298 $ & $ -0.2009929 $ \\ \cline{2-5}
 & $ 0.2011988 $ & $ -0.05619854 $ & $ 0.1663436 $ & $ -0.02250085 $ \\ \cline{2-5}
 & $ -0.1118389 $ & $ 0.3317198 $ & $ 0.5982598 $ & $ 0.247697 $ \\ \cline{2-5}
 & $ 0.5610356 $ & $ 0.9155656 $ & $ 0.9877718 $ & $ 0.7464178 $ \\ \hline
\end{tabular}
\end{center}

Najbardziej ryzykowne s¹ spó³ki z du¿ym $\dfrac{BV}{MV}$ (widaæ tu tak¹ zale¿noœæ monotoniczn¹).

\bigskip

PrzejdŸmy teraz do testu restrykcji GRS o hipotezie:

$$
\left\{
\begin{array}{l}
H: \alpha = 0 \\
K: \alpha \neq 0\\
\end{array}
\right.
$$

$P-value$ tego testu jest du¿e i wynosi $0.051$, zatem nie mamy podstaw do odrzucenia hipotezy zerowej, czyli $\alpha = 0$. Oznacza³oby to tyle, ¿e nasz model trójczynnikowy dobrze opisuje polsk¹ gie³dê, gdy¿ nie istnieje w nim element losowoœci, przypadkowoœci. SprawdŸmy jeszcze jednak, czy dobrze opisuje on te¿ drugi zestaw danych. Tym razem $p-value$ testu GRS jest ju¿ ma³e i wynosi $0.004$, tak wiêc odrzucamy hipotezê. Model zatem nie sprawdza siê dla drugiego zestawu danych. Model ten wiêc Ÿle opisuje polski rynek. 

\bigskip

PrzejdŸmy zatem do modelu czteroczynnikowego.

\subsection*{Model czteroczynnikowy}

Rozwa¿my model trójczynnikowy:
$$
R_{i,t}=\alpha_i + \beta_{RM,i}RM_t+ \beta_{SMB,i}SMB_t+ \beta_{HML,i}HML_t+\beta_{WML,i}WML_t+ \varepsilon_{i,t},
$$
gdzie $i=1,\ldots,16$, $t=1,\ldots,97$.

Dopasowaliœmy wiêc w ten sposób $16$ portfeli dla pierwszego zestawu danych.

Heteroskedastycznoœæ badaliœmy testem Breuscha-Pagana i ten wykaza³ heteroskedastycznoœæ w dwóch portfelach, natomiast test White'a wykry³ heteroskedastycznoœæ w piêciu portfelach. Nale¿y wiêc stwierdziæ, ¿e heteroskedastycznoœæ jest obecna. Co do autokorelacji, to, przy pomocy testu Durbina-Watsona, zosta³a ona wykryta tylko w dwóch przypadkach, czyli problem autokorelacji w modelu mo¿emy pomin¹æ. Mimo wszystko, ze wzglêdu na heteroskedastycznoœæ, trzeba bêdzie skorzystaæ z poprawki Newey-Westa.  

Zobaczmy teraz w jaki sposób uk³adaj¹ siê wartoœci estymatorów $\beta$:

\bigskip

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{RM}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ 0.9232306 $ & $ 0.8641042 $ & $ 0.8241291 $ & $ 0.8427869 $ \\ \cline{2-5}
 & $ 0.8479656 $ & $ 0.8568631 $ & $ 0.8152669 $ & $ 1.026009 $ \\ \cline{2-5}
 & $ 1.061216 $ & $ 0.9249988 $ & $ 0.8416023 $ & $ 1.092635 $ \\ \cline{2-5}
 & $ 0.8677172 $ & $ 0.8730714 $ & $ 0.9079475 $ & $ 0.9748165 $ \\ \hline
\end{tabular}
\end{center}

Przy $\beta_{RM}$ nie widaæ zbytniej zale¿noœci.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{SMB}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ 1.343542 $ & $ 1.100427 $ & $ 0.3916307 $ & $ 0.011027 $ \\ \cline{2-5}
 & $ 1.477718 $ & $ 0.9722053 $ & $ 0.2743222 $ & $ 0.2040493 $ \\ \cline{2-5}
 & $ 1.325322 $ & $ 0.809674 $ & $ 0.1153156 $ & $ -0.1269254 $ \\ \cline{2-5}
 & $ 1.171849 $ & $ 0.7647007 $ & $ 0.06432592 $ & $ -0.4343829 $ \\ \hline
\end{tabular}
\end{center}

$\beta_{RM}$ uk³ada siê monotonicznie zarówno ze wzglêdu na kapitalizacjê, jak i ze wzglêdu na $\dfrac{BV}{MV}$. Przy czym najmniej ryzykowne s¹ du¿e spó³ki z du¿ym $\dfrac{BV}{MV}$, a najbardziej ryzykowne ma³e spó³ki z ma³ym $\dfrac{BV}{MV}$ (rynek trochê je przecenia, dlatego s¹ ryzykowne).

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{HML}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ -0.4544298 $ & $ -0.6670027 $ & $ -0.09287146 $ & $ -0.2589012 $ \\ \cline{2-5}
 & $ 0.1046171 $ & $ 0.03900907 $ & $ 0.1133629 $ & $ -0.1123617 $ \\ \cline{2-5}
 & $ -0.03573682 $ & $ 0.3355533 $ & $ 0.502126 $ & $ 0.2849643 $ \\ \cline{2-5}
 & $ 0.4221964 $ & $ 0.8240721 $ & $ 0.932782 $ & $ 0.7199789 $ \\ \hline
\end{tabular}
\end{center}

$\beta_{RM}$ uk³ada siê w miarê monotonicznie ze wzglêdu na $\dfrac{BV}{MV}$. I tym razem najbardziej ryzykowne s¹ du¿e spó³ki z du¿ym $\dfrac{BV}{MV}$.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\beta_{WML}$ & \multicolumn{4}{|c|}{kapitalizacja $\longrightarrow$}  \\ \hline 
\multirow{4}{*}{$\dfrac{BV}{MV} \downarrow$} &  $ -0.1596162 $ & $ -0.2445455 $ & $ -0.01461578 $ & $ -0.07028727 $ \\ \cline{2-5}
 & $ -0.1172278 $ & $ 0.1155599 $ & $ -0.06430635 $ & $ -0.1090702 $ \\ \cline{2-5}
 & $ 0.09237024 $ & $ 0.004652982 $ & $ -0.1166842 $ & $ 0.04523386 $ \\ \cline{2-5}
 & $ -0.1685186 $ & $ -0.1110518 $ & $ -0.06674487 $ & $ -0.03209072 $ \\ \hline
\end{tabular}
\end{center}

\bigskip

PrzejdŸmy teraz do testu restrykcji GRS o hipotezie:

$$
\left\{
\begin{array}{l}
H: \alpha = 0 \\
K: \alpha \neq 0\\
\end{array}
\right.
$$

$P-value$ tego testu jest du¿e i wynosi $0.19$, zatem nie mamy podstaw do odrzucenia hipotezy zerowej. Model wiêc dobrze opisuje polsk¹ gie³dê. SprawdŸmy jednak, czy dobrze zachowuje siê równie¿ w przypadku drugiego zestawu danych, bo byæ mo¿e, tak jak w przypadku modelu trójczynnikowego, nie bêdzie siê dobrze zachowywa³ na nowych danych. $P-value$ wynosi $0.23$. Jest wiêc du¿e i znów nie daje podstaw do odrzucenia hipotezy zerowej. Model czteroczynnikowy sprawdza siê wiêc w przypadku polskiej gie³dy. 

\bigskip

Spróbujmy wiêc oszacowaæ premiê za ryzyko w tym modelu.

$$
\mathbb{E}(R_{t})=\gamma_0 + \gamma_{RM}\beta_{RM}+ \gamma_{SMB}\beta_{SMB}+ \gamma_{HML}\beta_{HML}+\gamma_{WML}\beta_{WML}
$$
 
Metod¹ wa¿onych najmniejszych kwadratów otrzymujemy nastêpuj¹ce wyniki:

\begin{eqnarray*}
\gamma_0 &=& -0.0327659584 \\
\gamma_{RM}&=& 0.0273126639\\
\gamma_{SMB}&=& 0.0017693357\\
\gamma_{HML}&=& 0.0066998285\\
\gamma_{WML}&=& -0.0005393353\\
\end{eqnarray*}

Przeprowadzaj¹c test istotnoœci wspó³czynników wysz³o, ¿e tylko $\gamma_{RM}$ jest niezerowy. Czyli tylko on, gdy siê zwiêkszy bêdzie mia³ wp³yw na premiê za ryzyko.

\begin{small}
<<echo=TRUE,eval=FALSE,message=FALSE,warning=FALSE>>=
library("bstats")
library("lmtest")
library("FinTS")

# model jednoczynnikowy:

p1 <- read.csv2("C:\\Users\\Marta\\Desktop\\Marta\\studia\\rok4\\Ekonometria\\spr3\\portfele_z1.csv",sep=";",header=TRUE)
head(p1)

wig1 <- p1[,2]

l1 <- vector("list",16)
for(i in 1:16){
   x <- p1[,i+5]
   l1[[i]] <- lm(x~wig1)
}
l1

w1 <- numeric(16)
bg <- numeric(16)
gq <- numeric(16)
dw <- numeric(16)
for(i in 1:16){
   w1[i] <- ifelse(white.test(l1[[i]])$p.value<0.05,1,0)
   bg[i] <- ifelse(bptest(l1[[i]])$p.value<0.05,1,0)
   gq[i] <- ifelse(gqtest(l1[[i]], fraction=0.33, order.by=~wig1)$p.value<0.05,1,0)
   dw[i] <- ifelse(dwtest(l1[[i]],alternative="two.sided")$p.value<0.05,1,0)
}

w1 # heteroskedastycznosc
bg # heteroskedastycznosc
gq # heteroskedastycznosc
dw # autokorelacja


rm1 <- numeric(0)
for(i in 1:16){
   rm1[i] <- l1[[i]]$coefficients[2]
}

# tabele dla RM:

rm1
tabela1_rm <- matrix(rm1,nrow=4)
tabela1_rm

t <- nrow(p1)
n <- 16
k <- 1

alfa1 <- numeric(16)
for(i in 1:16){
   alfa1[i] <- l1[[i]]$coefficients[1]
}
alfa1

m <- matrix(0,nrow=97,ncol=16)
for(i in 1:16){
   a <- l1[[i]]$residuals
   for(j in 1:97){
      m[j,i] <- a[j]
   }
}
m
dim(m)

sigma <- (t(m)%*%m)/t
sigma

head(p1)
v <- var(p1$WIG)

mi <- mean(p1$WIG)

grs <- (t/n)*((t-n-k)/(t-k-1))*((alfa1%*%solve(sigma)%*%t(t(alfa1)))/(1+mi%*%solve(v)%*%t(t(mi))))
grs
1-pf(grs,n,t-n-k)   # ma³e -> odrzucamy hipoteze, czyli alfa nie sa zerami


# model trójczynnikowy:

l2 <- vector("list",16)
for(i in 1:16){
   x <- p1[,i+5]
   l2[[i]] <- lm(x~wig1+p1$SMB+p1$HML)
}

w1 <- numeric(16)
bg <- numeric(16)
gq <- numeric(16)
dw <- numeric(16)
for(i in 1:16){
   w1[i] <- ifelse(white.test(l2[[i]])$p.value<0.05,1,0)
   bg[i] <- ifelse(bptest(l2[[i]])$p.value<0.05,1,0)
   gq[i] <- ifelse(gqtest(l2[[i]], fraction=0.33, order.by=~wig1)$p.value<0.05,1,0)
   dw[i] <- ifelse(dwtest(l2[[i]],alternative="two.sided")$p.value<0.05,1,0)
}
w1
bg
gq
dw

rm2 <- numeric(0)
smb2 <- numeric(0)
hml2 <- numeric(0)

for(i in 1:16){
   rm2[i] <- l2[[i]]$coefficients[2]
   smb2[i] <- l2[[i]]$coefficients[3]
   hml2[i] <- l2[[i]]$coefficients[4]
}

tabela2_rm <- matrix(rm2,nrow=4)
tabela2_smb <- matrix(smb2,nrow=4)
tabela2_hml <- matrix(hml2,nrow=4)

tabela2_rm 
tabela2_smb 
tabela2_hml


t <- nrow(p1)
n <- 16
k <- 3

alfa2 <- numeric(16)
for(i in 1:16){
   alfa2[i] <- l2[[i]]$coefficients[1]
}
alfa2

m <- matrix(0,nrow=97,ncol=16)
for(i in 1:16){
   a <- l2[[i]]$residuals
   for(j in 1:97){
      m[j,i] <- a[j]
   }
}
sigma2 <- (t(m)%*%m)/t

h <- matrix(c(p1$WIG,p1$SMB,p1$HML),nrow=97)

v <- cov(h)
mi <- apply(h,2,mean)
mi
grs <- (t/n)*((t-n-k)/(t-k-1))*((alfa2%*%solve(sigma2)%*%t(t(alfa2)))/
                                   (1+mi%*%solve(v)%*%t(t(mi))))
grs
1-pf(grs,n,t-n-k)


# dla p2

l22 <- vector("list",16)
for(i in 1:16){
   x <- p2[,i+5]
   l22[[i]] <- lm(x~wig1+p2$SMB+p2$HML)
}

w1 <- numeric(16)
bg <- numeric(16)
gq <- numeric(16)
dw <- numeric(16)
for(i in 1:16){
   w1[i] <- ifelse(white.test(l22[[i]])$p.value<0.05,1,0)
   bg[i] <- ifelse(bptest(l22[[i]])$p.value<0.05,1,0)
   gq[i] <- ifelse(gqtest(l22[[i]], fraction=0.33, order.by=~wig1)$p.value<0.05,1,0)
   dw[i] <- ifelse(dwtest(l22[[i]],alternative="two.sided")$p.value<0.05,1,0)
}
w1
bg
gq
dw

rm2 <- numeric(0)
smb2 <- numeric(0)
hml2 <- numeric(0)

for(i in 1:16){
   rm2[i] <- l22[[i]]$coefficients[2]
   smb2[i] <- l22[[i]]$coefficients[3]
   hml2[i] <- l22[[i]]$coefficients[4]
}

tabela2_rm <- t(matrix(rm2,nrow=4))
tabela2_smb <- t(matrix(smb2,nrow=4))
tabela2_hml <- t(matrix(hml2,nrow=4))

tabela2_rm 
tabela2_smb 
tabela2_hml


t <- nrow(p2)
n <- 16
k <- 3

alfa2 <- numeric(16)
for(i in 1:16){
   alfa2[i] <- l22[[i]]$coefficients[1]
}
alfa2

m <- matrix(0,nrow=97,ncol=16)
for(i in 1:16){
   a <- l22[[i]]$residuals
   for(j in 1:97){
      m[j,i] <- a[j]
   }
}
sigma2 <- (t(m)%*%m)/t

h <- matrix(c(p1$WIG,p1$SMB,p1$HML),nrow=97)

v <- cov(h)
mi <- apply(h,2,mean)
mi
grs <- (t/n)*((t-n-k)/(t-k-1))*((alfa2%*%solve(sigma2)%*%t(t(alfa2)))/
                                   (1+mi%*%solve(v)%*%t(t(mi))))
grs
1-pf(grs,n,t-n-k)   # odrzucamy hipotezê

###########################################################################
# model czteroczynnikowy:

l3 <- vector("list",16)
for(i in 1:16){
   x <- p1[,i+5]
   l3[[i]] <- lm(x~wig1+p1$SMB+p1$HML+p1$WML)
}

l3

w1 <- numeric(16)
bg <- numeric(16)
gq <- numeric(16)
dw <- numeric(16)
for(i in 1:16){
   w1[i] <- ifelse(white.test(l3[[i]])$p.value<0.05,1,0)
   bg[i] <- ifelse(bptest(l3[[i]])$p.value<0.05,1,0)
   gq[i] <- ifelse(gqtest(l3[[i]], fraction=0.33, order.by=~wig1)$p.value<0.05,1,0)
   dw[i] <- ifelse(dwtest(l3[[i]],alternative="two.sided")$p.value<0.05,1,0)
}

w1 # heteroskedastycznosc
bg # heteroskedastycznosc
gq # heteroskedastycznosc
dw # autokorelacja


rm3 <- numeric(0)
smb3 <- numeric(0)
hml3 <- numeric(0)
wml3 <- numeric(0)

for(i in 1:16){
   rm3[i] <- l3[[i]]$coefficients[2]
   smb3[i] <- l3[[i]]$coefficients[3]
   hml3[i] <- l3[[i]]$coefficients[4]
   wml3[i] <- l3[[i]]$coefficients[5]
}


tabela3_rm <- matrix(rm3,nrow=4)
tabela3_smb <- matrix(smb3,nrow=4)
tabela3_hml <- matrix(hml3,nrow=4)
tabela3_wml <- matrix(wml3,nrow=4)


tabela3_rm
tabela3_smb
tabela3_hml
tabela3_wml

t <- nrow(p1)
n <- 16
k <- 4

alfa3 <- numeric(16)
for(i in 1:16){
   alfa3[i] <- l3[[i]]$coefficients[1]
}
alfa3

m <- matrix(0,nrow=97,ncol=16)
for(i in 1:16){
   a <- l3[[i]]$residuals
   for(j in 1:97){
      m[j,i] <- a[j]
   }
}
sigma3 <- (t(m)%*%m)/t

h <- matrix(c(p1$WIG,p1$SMB,p1$HML,p1$WML),nrow=97)

v <- cov(h)
mi <- apply(h,2,mean)
mi
grs <- (t/n)*((t-n-k)/(t-k-1))*((alfa3%*%solve(sigma3)%*%t(t(alfa3)))/(1+mi%*%solve(v)%*%t(t(mi))))
grs
1-pf(grs,n,t-n-k)

# dla p2:

l33 <- vector("list",16)
for(i in 1:16){
   x <- p2[,i+5]
   l33[[i]] <- lm(x~p2$WIG+p2$SMB+p2$HML+p2$WML)
}

t <- nrow(p2)
n <- 16
k <- 4

alfa3 <- numeric(16)
for(i in 1:16){
   alfa3[i] <- l33[[i]]$coefficients[1]
}
alfa3

m <- matrix(0,nrow=97,ncol=16)
for(i in 1:16){
   a <- l33[[i]]$residuals
   for(j in 1:97){
      m[j,i] <- a[j]
   }
}
sigma3 <- (t(m)%*%m)/t

h <- matrix(c(p2$WIG,p2$SMB,p2$HML,p2$WML),nrow=97)

v <- cov(h)
mi <- apply(h,2,mean)
mi
grs <- (t/n)*((t-n-k)/(t-k-1))*((alfa3%*%solve(sigma3)%*%t(t(alfa3)))/(1+mi%*%solve(v)%*%t(t(mi))))
grs
1-pf(grs,n,t-n-k)

# 6
# dwoch pierwszych nie ma sensu - tylko trzeci

# pierwsza metoda: (wazona metoda najmniejszych kwadratow)

l3

xx <- matrix(0,nrow=16,ncol=5)
xx[,1] <- 1
for(i in 1:16){
   xx[i,2:5] <- l3[[i]]$coefficients[2:5]
}

xx
sigma3

p1

r_sr <- apply(p1[6:21],2,mean) 
r_sr

gamma <- solve(t(xx)%*%solve(sigma3)%*%xx)%*%t(xx)%*%solve(sigma3)%*%r_sr
gamma  # to jest premia za ryzyko -> srednio 0.2 procenta

lm(r_sr~xx+0 ,weights=diag(sigma3))

va <- (1/97)*( solve( t(xx)%*%solve(sigma3)%*%xx ) + rbind(0,cbind(0,v)) )
va

gamma[1]/sqrt(va[1,1])
1-pt(gamma[1]/sqrt(va[1,1]),97-5)  # gammma 0 wyszlo rowne zero
qt(0.95,97-5)

gamma[2]/sqrt(va[2,2])
1-pt(gamma[2]/sqrt(va[2,2]),97-5)

gamma[3]/sqrt(va[3,3])
1-pt(gamma[3]/sqrt(va[3,3]),97-5)

gamma[4]/sqrt(va[4,4])
1-pt(gamma[4]/sqrt(va[4,4]),97-5)

gamma[5]/sqrt(va[5,5])
1-pt(gamma[5]/sqrt(va[5,5]),97-5)

# jesli hml zwiekszy sie o jeden procent, to ma to wplyw na wartosc naszego portfele, jesli sie zwieksza pozostale to juz raczej nie

# druga metoda:

eps <- numeric(5*16)
mac <- matrix(0,nrow=80,ncol=97)

for(j in 1:97){
   
   for(i in 1:16){
      eps[i] <- l3[[i]]$residuals[j]
   }
   
   eps[17:32] <- eps[1:16]*p1[j,2]
   eps[33:48] <- eps[1:16]*p1[j,3]
   eps[49:64] <- eps[1:16]*p1[j,4]
   eps[65:80] <- eps[1:16]*p1[j,5]
   
   mac[,j] <- eps
}

mac

gt <- apply(mac,1,sum)
gt
@
 
\end{small} 
 
\end{document}